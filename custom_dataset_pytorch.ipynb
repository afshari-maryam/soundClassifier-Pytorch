{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_dataset_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Audio Pytorch dataset with Pytorch & torchaudio "
      ],
      "metadata": {
        "id": "ToEoOGbGNiPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_6hhxYt1HMi",
        "outputId": "d1af94ea-cb9c-46bb-91c1-36cd7634a5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-19 23:17:17--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘urban8k.tgz’\n",
            "\n",
            "urban8k.tgz         100%[===================>]   5.61G  13.4MB/s    in 5m 13s  \n",
            "\n",
            "2022-03-19 23:22:32 (18.3 MB/s) - ‘urban8k.tgz’ saved [6023741708/6023741708]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Unzip dataset\n",
        "!wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
        "!tar -xzf urban8k.tgz\n",
        "!rm urban8k.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"/content/UrbanSound8K\" \"/content/drive/MyDrive/Afshari/colab test/Test_USD\""
      ],
      "metadata": {
        "id": "WV5R1U7vgFrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import dataset\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "import os"
      ],
      "metadata": {
        "id": "-88D500vfA4X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UrbanSoundDataset():\n",
        "\n",
        "  def __init__(self,annotations_file, audio_dir, transformation, target_sample_rate):\n",
        "\n",
        "    self.annotations = pd.read_csv(annotations_file)\n",
        "    self.audio_dir = audio_dir\n",
        "    self.transformation = transformation\n",
        "    self.target_sample_rate = target_sample_rate\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    audio_sample_path = self._get_audio_sample_path(index)\n",
        "    label = self._get_audio_sample_label(index)\n",
        "    signal, sr = torchaudio.load(audio_sample_path)\n",
        "    signal = self._resample_if_necessary(signal,sr)\n",
        "    #signal -> (num_channels,samples) -> (2,16000) -> (1,16000)\n",
        "    signal = self._mix_down_if_necessary(signal)\n",
        "    signal = self.transformation(signal)\n",
        "    return signal, label\n",
        "\n",
        "  def _resample_if_necessary(self,signal,sr):\n",
        "    if sr != self.target_sample_rate:\n",
        "      resampler = torchaudio.transforms.Resample(sr,self.target_sample_rate)\n",
        "      signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "  def _mix_down_if_necessary(self,signal):\n",
        "    if signal.shape[0] >1: #(2,16000)\n",
        "      signal = torch.mean(signal,dim=0,keepdim=True)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  def _get_audio_sample_path(self,index):\n",
        "    fold = f\"fold{self.annotations.iloc[index,5]}\"\n",
        "    path = os.path.join(self.audio_dir,fold, self.annotations.iloc[index,0])\n",
        "    return path\n",
        "\n",
        "  def _get_audio_sample_label(self,index):\n",
        "    return self.annotations.iloc[index,6]\n",
        "  \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  ANNOTATIONS_FILE =\"/content/drive/MyDrive/Afshari/Part2/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
        "  AUDIO_DIR = \"/content/drive/MyDrive/Afshari/Part2/UrbanSound8K/audio\"\n",
        "  SAMPLE_RATE = 16000\n",
        "\n",
        "  mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate= SAMPLE_RATE,\n",
        "        n_fft= 1024,\n",
        "        hop_length= 512,\n",
        "        n_mels=64\n",
        "        )\n",
        "    #ms = mel_spectrogram(signal)\n",
        "  usd =UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE )\n",
        "\n",
        "  print(f\"There are {len(usd)} samples in the dataset.\")\n",
        "\n",
        "  signal , label = usd[0]\n",
        "\n",
        "  print(f\"label = {label}\")\n",
        "\n",
        "  print(f\"signal = {signal.size()}\")\n",
        "\n",
        "  a=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVMfW4nEfSZn",
        "outputId": "ee9ae62e-b892-4bed-c893-f906d4d76a3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8732 samples in the dataset.\n",
            "label = 3\n",
            "signal = torch.Size([1, 64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Email:afshari1431@gmail.com\n",
        "#afshari@ce.sharif.edu"
      ],
      "metadata": {
        "id": "Fnuf1ujZ8xEz"
      }
    }
  ]
}